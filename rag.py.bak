pip install langchain
pip install torch
pip install transformers
pip install sentence-transformers
pip install datasets
pip install faiss-cpu
from langchain.document_loaders import HuggingFaceDatasetLoader
from langchin.text_splitter import RecursiveCharacterTextSplitter
from langchain.embedding import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from transformers import AutoTokenizer, AutoModelQuestionAnswering
from transforers import AutoTokenizer, pipeline
from langchain import HuggingFcaePipeline
from langchain.chains import RetriealQA
# Specify the dataset name and the column containing the content
dataset_name = "speaker.mp4"
page_content_column = "context"  # or any other column you're intersted in

# Create a loader instance
loader = HuggingFaceDatasetLoader(dataset_name,page_content_column)


# Load te data
data = loader.load()


# Display the first entries
data[:2]

# Create an intance of the RecursiveCharacterTextSplitter class with specific parameters.
# It splits text into chucks of 1000 charcaters each with a 150-character overlap
text_splitter = RecursiveCharacterTextSplitter(chuck_size=1000, chunk_overlap=150)


# 'data' holds the text you want to split, split the text into documents using the text splitter
docs = text_splitter.split_documnets(data)

docs[0]

# Define the path to the pre-trained model you want to use
modelPath = "sentence-transformers/all-MiniLM-l6-v2"

# Create a dictionary with encoding options, specifically setting  to use the CPU for computations
encode_kwargs = {'device':'cpu'}

# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False
encode_kwags = {'normalize_embeddings':False}

# Initalize an instance of HuggingFaceEmbedings with specificied parameters
embeddings = HuggingFaceEmbeddiings(
         model_name=modelpath,      # Provide the pre-trained model's path
		 model_kwargs=model_kwargs, # Pass the model configuration options
		 encode_kwargs=encode_kwargs # Pass the encoding options


text = "This is a test document."
query_result = embeddings.embed_query(text)
query_result[:3]

db = FAISS.from_documents(docs, embeddings)

question = "What is cheesemaking?"
searchDocs = db.similarity_search(question)
print(searchDocs[0].page_content)


# Create a tokenizer object by loading the pretrained "Intel/dynamic_tinybert" tokenizer.
tokenizer = AutoTokenizer.from_pretrained("Intel/dynamic_tinybert")

# Create a question-answering model object by loading the pretrained'Intel/dynamic_tinybert" model.
model = AutoModelForQuestionAnswering,from_pretrained("Intel/dynaic_tinybert")

# Specify the model name you want to use
model_name = "Intel/dynamic_tinybert"

# Load the tokenizer associated with the specified model
tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)

# Define a question-answering pipeline using the model and tokenizer
question_answerer = pipeline(
      "question-answering",
	   model=model_name,
	   tokenizer=tokenizer,
	   return_tensors='pt'
	   )

# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline
# with additional model-specific arguments(temperature and max_length)
llm = HuggingFacePipeline(
     pipeline=question_answerer,
	 model_kwargd={"temperature":0.7, "max_length":512},
	 )

docs = retriever.get_relevant_documents("What is Cheesemaking?")
print(docs[0].page_content)

# Create a retriever object from the 'db' with a search configration where it retrieves up to 4 relevant splits/documents.
retriever = db.as_retriever(search_kwargs={"K":4})

# Create a question-answering instance (qa) using the RetrievalQA class
# It's configured with a language model (llm) , a chain type "refine"